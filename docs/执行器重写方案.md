#### 参考SparkSQL 源码

SparkSQL 关于 SQL 编译器前端的实现是基于Antlr 4实现的, 文件名称叫做`SqlBase.g4` 其中代码片段如下所示:

```text
insertInto
    : INSERT OVERWRITE TABLE? multipartIdentifier (partitionSpec (IF NOT EXISTS)?)?  identifierList?        #insertOverwriteTable
    | INSERT INTO TABLE? multipartIdentifier partitionSpec? (IF NOT EXISTS)? identifierList?                #insertIntoTable
    | INSERT OVERWRITE LOCAL? DIRECTORY path=STRING rowFormat? createFileFormat?                            #insertOverwriteHiveDir
    | INSERT OVERWRITE LOCAL? DIRECTORY (path=STRING)? tableProvider (OPTIONS options=tablePropertyList)?   #insertOverwriteDir
    ;
```

其中`#` 分隔符后面的是在`java/scala`文件中的标识符信息, Antlr会根据这个标识符生成对应的上下文`xxxContext`类, 根据这个类来构建逻辑执行计划. 

#### 关于Parser

SparkSQL 的parser 在目录`org.apache.spark.sql.parser`下面, 其中核心类为`AstBuilder`, 其余类是辅助类. 这个类主要使用的是访问者模式， 实现对SQL中token的访问, 其继承了`SqlBaseBaseVisitor`类, 这个类是需要通过Antlr 编译才会出现的类. 是通过Antlr 自动生成的类，这个类提供了大量token访问的方法，例如:

```scala
 override def visitSingleFunctionIdentifier(
      ctx: SingleFunctionIdentifierContext): FunctionIdentifier = withOrigin(ctx) {
    visitFunctionIdentifier(ctx.functionIdentifier)
  }

override def visitSingleDataType(ctx: SingleDataTypeContext): DataType = withOrigin(ctx) {
    typedVisit[DataType](ctx.dataType)
  }
```

这些上下文信息, 起始就是上文所说# 后面的别名信息

所以对`SqlBase.g4`文件的修改需要对应反应到这个`Parser`类中. 否则会使用Antlr默认的实现来进行填充.



#### 关于逻辑计划

逻辑执行计划位于`org.apache.spark.sql.catalyst.plans.logic`包下, 核心实现为`QueryPlan`. 

```scala
abstract class QueryPlan[PlanType <: QueryPlan[PlanType]]
  extends TreeNode[PlanType] with SQLConfHelper
```

同时也是一个树状的结构形式, 满足火山引擎的要求，即下层为上次操作提供操作数。

即, 提供了对应的输入和输出

```scala
def output: Seq[Attribute]

lazy val outputSet: AttributeSet = AttributeSet(output)
```

`QueryPlan`不仅仅可以表示逻辑计划, 而且可以表示物理执行计划, 更细粒度的来说, 实现逻辑计划的类为`LogiclPlan`.

```scala
abstract class LogicalPlan
  extends QueryPlan[LogicalPlan]
  with AnalysisHelper
  with LogicalPlanStats
  with QueryPlanConstraints
  with Logging
```

同时`LogicPlan`也是满足火山引擎结构的. 分为叶子节点, 二元节点(含两个操作数), 一元节点(一个操作符).

#### 关于代码生成

代码生成器位于`org.apache.spark.sql.catalyst.expressions.codegen`目录下,  这里主要负责生成SQL中各个逻辑部分的Java代码, 调用这些代码, 控制底层执行层进行存储操作, 完成SQL逻辑的执行。修改的时候, 遵循开闭原则, 添加自己的代码生成逻辑即可.

同时`org.apache.spark.sql.catalyst.expressions`包含各种常用表达式的逻辑执行计划结构. 

#### 关于执行计划

执行层根据不同的数据库类型, 其执行层的逻辑页不尽相同。需要结合对应的API去处理执行层的逻辑, 可以使用Spring框架或者是其他第三方库, 例如`jedis`提供给我们的操作API实现执行器的逻辑.



#### 文件结构设计

根据上述调研, 以`SqlBase.g4`为核心, 需要我们自己去写`Parser`去适配Antlr文件, 基于此也需要去修改相关的代码生成模块, 用于适配实现的逻辑。代码生成中需要包含执行器层的逻辑.

```text
---- lotus-common
|--- antlr
||-- rules
|||- SqlBase.g4
|--- com.ledis
||-- com.ledis.codegen
||-- com.ledis.parser
||-- com.ledis.executor
```

需要使用SparkSQL源码的内容

```java
org.apach.spark.sql.catalyst.expressions;
org.apach.spark.sql.catalyst.parser.*;
org.apach.spark.sql.catalyst.trees.*;
org.apach.spark.sql.catalyst.analysis.*;
org.apach.spark.sql.catalyst.planning.*;
org.apach.spark.sql.catalyst.plans.*;
org.apach.spark.sql.catalyst.rules.*;
```



##### 如何将Scala文件转换为Java文件

项目中使用Java语言进行编码, 但是SparkSQL源码大多数是Scala实现的, 所以就需要进行语言间的转换.



